{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SQL Query using Pyspark**"
      ],
      "metadata": {
        "id": "dxWdya59OaFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "byevWzILWo6Q"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ura4SiZGWzJb"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P92TioFDYN8f",
        "outputId": "31455f5f-bdf4-4196-9c2e-d67dfc52d4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-22 03:59:28--  https://github.com/tmclouisluk/FIFA-players-Data-Analysis/blob/master/pre_model_df.csv\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5403 (5.3K) [text/plain]\n",
            "Saving to: ‘pre_model_df.csv’\n",
            "\n",
            "pre_model_df.csv    100%[===================>]   5.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-22 03:59:28 (65.9 MB/s) - ‘pre_model_df.csv’ saved [5403/5403]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget https://github.com/tmclouisluk/FIFA-players-Data-Analysis/blob/master/pre_model_df.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl4InyjSYaAQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SQLonPyspark\").getOrCreate()\n",
        "data = spark.read.csv(\"/content/sample_data/california_housing_train.csv\", header=True, inferSchema=True)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paRhEdybfsqi"
      },
      "source": [
        "# **SELECT QEURY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68viSUeSamfb",
        "outputId": "3683253e-c2be-4190-86a7-2c87bb4aff7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------+\n",
            "|total_rooms|population|\n",
            "+-----------+----------+\n",
            "|     5612.0|    1015.0|\n",
            "|     7650.0|    1129.0|\n",
            "|      720.0|     333.0|\n",
            "|     1501.0|     515.0|\n",
            "|     1454.0|     624.0|\n",
            "+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.createOrReplaceTempView(\"housingdata\")\n",
        "result = spark.sql(\"SELECT total_rooms, population FROM housingdata\")\n",
        "result.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbOKhE2Xfxl4"
      },
      "source": [
        "# **Filter with WHERE clause**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17HwWurKfKOK",
        "outputId": "269ad7f1-3832-401d-ff16-6a527b00b41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+----------+\n",
            "|longitude|latitude|population|\n",
            "+---------+--------+----------+\n",
            "|  -114.56|   33.69|     333.0|\n",
            "|  -114.57|   33.64|     515.0|\n",
            "|  -114.57|   33.57|     624.0|\n",
            "|  -114.58|   33.63|     671.0|\n",
            "|  -114.59|   34.83|     375.0|\n",
            "+---------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_filtered = spark.sql(\"SELECT longitude, latitude, population FROM housingdata WHERE population < 1000\")\n",
        "result_filtered.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTN9VNwrg_Cl"
      },
      "source": [
        "# **Aggregation Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHaGwI8rg-O9",
        "outputId": "379ee07d-ad94-4c5b-979b-870ecbc3a2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|    avg_population|\n",
            "+------------------+\n",
            "|1429.5739411764705|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_aggregated = spark.sql(\"SELECT AVG(population) AS avg_population FROM housingdata\")\n",
        "result_aggregated.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Ak81_nhQi8"
      },
      "source": [
        "# **ORDER BY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txcBklVzhqE7",
        "outputId": "d1d2107d-2303-40ec-9065-4332d195e70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+-----------+\n",
            "|longitude|latitude|total_rooms|\n",
            "+---------+--------+-----------+\n",
            "|  -117.74|   33.89|    37937.0|\n",
            "|  -121.79|   36.64|    32627.0|\n",
            "|  -117.78|   34.03|    32054.0|\n",
            "|  -118.78|   34.16|    30405.0|\n",
            "|  -117.12|   33.52|    30401.0|\n",
            "+---------+--------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_ordered = spark.sql(\"SELECT longitude, latitude, total_rooms FROM housingdata ORDER BY total_rooms DESC\")\n",
        "result_ordered.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npbkjsB3h8Wz"
      },
      "source": [
        "# **JOINS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hHPXTYJiK9a",
        "outputId": "5a6c24b8-7526-4378-d8d6-c20a719c6872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+-----------+\n",
            "| name|age| department|\n",
            "+-----+---+-----------+\n",
            "|  Bob| 30|  Marketing|\n",
            "|Alice| 25|Engineering|\n",
            "+-----+---+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data2 = [(\"Alice\", \"Engineering\"), (\"Bob\", \"Marketing\"), (\"David\", \"Finance\")]\n",
        "columns2 = [\"name\", \"department\"]\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "df2.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
        "columns = [\"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "join_query = \"SELECT p.name, p.age, d.department FROM people p JOIN departments d ON p.name = d.name\"\n",
        "result_join = spark.sql(join_query)\n",
        "result_join.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZThTX-yiyHK"
      },
      "source": [
        "# **Sub Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nNTPrv0jJ-p",
        "outputId": "3f1e826c-6110-4cb6-c93d-39b51a08efd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------+----------+\n",
            "|total_rooms|total_bedrooms|population|\n",
            "+-----------+--------------+----------+\n",
            "|     2907.0|         680.0|    1841.0|\n",
            "|     4789.0|        1175.0|    3134.0|\n",
            "|     3741.0|         801.0|    2434.0|\n",
            "|     1706.0|         397.0|    3424.0|\n",
            "|     3414.0|         666.0|    2097.0|\n",
            "+-----------+--------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "subquery = \"SELECT total_rooms, total_bedrooms, population FROM housingdata WHERE population > (SELECT AVG(population) FROM housingdata)\"\n",
        "result_subquery = spark.sql(subquery)\n",
        "result_subquery.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlwwGv9UjjQ_"
      },
      "source": [
        "# **UPDATE Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Curq50ELji2f",
        "outputId": "39dfddc3-0a4a-438e-b76c-50171729acb4"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7e0f90810e00>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# update_query = \"UPDATE people SET age = 40 WHERE name = 'Bob'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mupdated_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: unresolved operator 'InsertIntoStatement LogicalRDD [name#5680, age#5681L], false, true, false;\n'InsertIntoStatement LogicalRDD [name#5680, age#5681L], false, true, false\n+- Project [name#5680, CASE WHEN (name#5680 = Bob) THEN cast(40 as bigint) ELSE age#5681L END AS age#5781L]\n   +- SubqueryAlias people\n      +- LogicalRDD [name#5680, age#5681L], false\n"
          ]
        }
      ],
      "source": [
        "# new_age = 40\n",
        "# update_condition = \"name = 'Bob'\"\n",
        "\n",
        "# update_query = f\"\"\"\n",
        "#     INSERT OVERWRITE TABLE people\n",
        "#     SELECT\n",
        "#         name,\n",
        "#         CASE WHEN {update_condition} THEN {new_age} ELSE age END AS age\n",
        "#     FROM people\n",
        "# \"\"\"\n",
        "\n",
        "# # update_query = \"UPDATE people SET age = 40 WHERE name = 'Bob'\"\n",
        "# spark.sql(update_query)\n",
        "\n",
        "# updated_df = spark.sql(\"SELECT * FROM people\")\n",
        "# updated_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3ngDoLVnKg8"
      },
      "source": [
        "# **Insert New Attributes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QZPZxAdmg6x",
        "outputId": "4deeb115-b48d-4a59-8e81-5188c00213a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original DataFrame:\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 22|\n",
            "+-------+---+\n",
            "\n",
            "DataFrame with New Attributes:\n",
            "+-------+---+-----------+\n",
            "|   name|age| department|\n",
            "+-------+---+-----------+\n",
            "|  Alice| 25|Engineering|\n",
            "|    Bob| 30|Engineering|\n",
            "|Charlie| 22|Engineering|\n",
            "+-------+---+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
        "columns = [\"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "new_attributes_query = \"\"\"\n",
        "    SELECT\n",
        "        name,\n",
        "        age,\n",
        "        'Engineering' AS department  -- New department column with constant value\n",
        "    FROM people\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query to create a new DataFrame with the new column\n",
        "new_attributes_df = spark.sql(new_attributes_query)\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(\"DataFrame with New Attributes:\")\n",
        "new_attributes_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMgcosDIpD8t"
      },
      "source": [
        "# **Date and Time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWd2BE6rpUn",
        "outputId": "0c6e8f3c-339b-44bb-a120-f074234818b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "| id|     input|\n",
            "+---+----------+\n",
            "|  1|2020-02-01|\n",
            "|  2|2019-03-01|\n",
            "|  3|2021-03-01|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
        "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFxZpFAssCny"
      },
      "source": [
        "# **Current Date Show**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF-uy0WFr-Ht",
        "outputId": "f6ff1821-ef56-43ab-cdba-3fa881a89d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|current_date|\n",
            "+------------+\n",
            "|  2023-08-22|\n",
            "+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(current_date().alias(\"current_date\")\n",
        "  ).show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9_ZjqelsGVW"
      },
      "source": [
        "# **Date Formating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2WEhylWsGC-",
        "outputId": "82a5fe9e-68c1-4aa2-e56a-d990c3bcc865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+\n",
            "|     input|date_format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 02-01-2020|\n",
            "|2019-03-01| 03-01-2019|\n",
            "|2021-03-01| 03-01-2021|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(col(\"input\"),\n",
        "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")\n",
        "  ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUqmYV3nsoH-"
      },
      "source": [
        "# **Date Difference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D6foh0Us3Ym",
        "outputId": "dadcfea7-c47b-4488-92ab-608d2c5ce711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+\n",
            "|     input|datediff|\n",
            "+----------+--------+\n",
            "|2020-02-01|    1298|\n",
            "|2019-03-01|    1635|\n",
            "|2021-03-01|     904|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df.select(col(\"input\"),\n",
        "    datediff(current_date(),col(\"input\")).alias(\"datediff\")\n",
        "  ).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFo6tFU5tX69"
      },
      "source": [
        "# **Day, Month, Year**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u61468_QtcwU",
        "outputId": "7e8f964b-0f1d-49e3-84a3-a8a5a8d7ca92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+-----+----------+\n",
            "|     input|year|month|weekofyear|\n",
            "+----------+----+-----+----------+\n",
            "|2020-02-01|2020|    2|         5|\n",
            "|2019-03-01|2019|    3|         9|\n",
            "|2021-03-01|2021|    3|         9|\n",
            "+----------+----+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(col(\"input\"),\n",
        "     year(col(\"input\")).alias(\"year\"),\n",
        "     month(col(\"input\")).alias(\"month\"),\n",
        "     weekofyear(col(\"input\")).alias(\"weekofyear\")\n",
        "  ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu-XC28utrGE"
      },
      "source": [
        "# **Timestamp**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7XNuEBFtixD",
        "outputId": "6fc2b0a3-fed0-4d64-ce75-22fe89e1e62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+----+------+------+\n",
            "|input                  |hour|minute|second|\n",
            "+-----------------------+----+------+------+\n",
            "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
            "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
            "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
            "+-----------------------+----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
        "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "\n",
        "df3.select(col(\"input\"),\n",
        "    hour(col(\"input\")).alias(\"hour\"),\n",
        "    minute(col(\"input\")).alias(\"minute\"),\n",
        "    second(col(\"input\")).alias(\"second\")\n",
        "  ).show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}